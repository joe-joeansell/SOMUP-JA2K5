-Clear mental model
-Possibly borrowing on existing knowledge, intuitive
# Portfolio Inclusions
PROMPT - LLM Input, result of input
CRITICAL FILTERING - Explanation of what was kept changed or discarded- Why?
REFLECTION - What did LLM help you understand/achieve

# Ideation
==I think i'd like to do something with customisable visualisers for music using Open Sound Control and Processing. Give me four ideas using these elements.==

Great idea, Joe! Using **Open Sound Control (OSC)** and **Processing** to create **customisable music visualisers** opens up a lot of creative and technical possibilities. Here are **four distinct project ideas** that align with your module's goals and assessment criteria:

---

### üé® 1. **Modular Visual Synth: A Customisable OSC-Controlled Visualiser**

**Concept**: A Processing-based visual synthesiser where users can dynamically switch between visual modules (e.g., waveform, particle, geometric, fluid) using OSC messages.

**Features**:

- OSC messages control module selection, colour schemes, and animation speed.
- Each module reacts differently to audio parameters like amplitude, frequency, or MIDI velocity.
- Users can build their own visual chains (like modular synths but for visuals).

**AI Collaboration Angle**:

- Use LLMs to generate shader code or optimise visual performance.
- Prompt AI to suggest mappings between sound features and visual behaviours.

---

### üåå 2. **Emotion Mapper: Visualising Music Mood via OSC**

**Concept**: A visualiser that interprets musical input (e.g., tempo, key, dynamics) and maps it to emotional states (e.g., calm, energetic, melancholic), then renders corresponding visuals.

**Features**:

- OSC messages carry musical metadata (e.g., from DAW or SuperCollider).
- Visuals change based on detected mood: colour palette, motion style, density.
- Users can override mood mappings via OSC sliders or presets.

**AI Collaboration Angle**:

- Use AI to help define mood categories and generate visual styles.
- Prompt AI to analyse musical features and suggest emotional mappings.

---

### üß© 3. **Interactive Visual Score: A Live Performance Tool**

**Concept**: A Processing canvas that acts as a visual score, reacting to live OSC input from performers and guiding them with evolving visual cues.

**Features**:

- OSC messages trigger visual motifs, shapes, or timelines.
- Can be used in ensemble settings where visuals cue different players.
- Customisable layout and interaction modes (e.g., grid, timeline, abstract).

**AI Collaboration Angle**:

- Use AI to generate visual motifs based on musical structure.
- Prompt AI to simulate ensemble scenarios and suggest cue strategies.

---

### üåÄ 4. **Generative Visual Playground: User-Curated Visual Worlds**

**Concept**: A sandbox-style visualiser where users can ‚Äúpaint‚Äù with sound using OSC messages to control brush types, colours, and behaviours.

**Features**:

- OSC input from MIDI controllers or mobile apps (e.g., TouchOSC).
- Visual elements grow, morph, or decay based on sound parameters.
- Users can save and load visual ‚Äúpresets‚Äù or ‚Äúworlds‚Äù.

**AI Collaboration Angle**:

- Use AI to generate brush behaviours or visual presets.
- Prompt AI to help design intuitive OSC mappings for expressive control.

Not hundred percent happy with these ideas for now, I think I will try further ideation and provide extra context to make it clearer what sort of thing I would like to create.